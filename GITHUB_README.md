# ğŸš— Vehicle Collision Damage Detection System

**AI-powered system for automatic detection and classification of vehicle damage using computer vision and deep learning.**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0+-ee4c2c.svg)](https://pytorch.org/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Architecture](#architecture)
- [Dataset](#dataset)
- [Installation](#installation)
- [Usage](#usage)
- [Training](#training)
- [API Reference](#api-reference)
- [Project Structure](#project-structure)
- [Results](#results)
- [Contributing](#contributing)
- [License](#license)

---

## ğŸ¯ Overview

This system automatically detects and classifies vehicle damage from images using state-of-the-art deep learning models. It can identify:

- **8 Damage Types**: Scratch, Dent, Cracked, Broken part, Missing part, Corrosion, Paint chip, Flaking
- **21 Vehicle Parts**: Front/Back bumper, Doors, Fenders, Hood, Trunk, Wheels, Lights, Windshield, etc.
- **Damage Severity**: Scored from 1-10
- **Damage Location**: Precise bounding boxes and segmentation masks

### Use Cases
- ğŸš— Insurance claim processing
- ğŸ”§ Auto repair estimation
- ğŸ“¸ Vehicle inspection automation
- ğŸ“Š Fleet damage tracking

---

## âœ¨ Features

- âœ… **Dual-Model Architecture**: YOLO11m for detection + ResNet50 for classification
- âœ… **Unified Transformer**: Single end-to-end model option (experimental)
- âœ… **COCO Format Annotations**: Industry-standard annotation format
- âœ… **Batch Processing**: Process thousands of images automatically
- âœ… **RESTful API**: FastAPI-based inference service
- âœ… **MLflow Integration**: Experiment tracking and model versioning
- âœ… **DVC Support**: Data version control
- âœ… **Docker Ready**: Containerized deployment

---

## ğŸ—ï¸ Architecture

### Option 1: YOLO + ResNet Pipeline (Production-Ready)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  YOLO11m Detector   â”‚
â”‚  - Damage detection â”‚
â”‚  - Part detection   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ResNet50 Classifierâ”‚
â”‚  - Damage type      â”‚
â”‚  - Severity score   â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Post-processing    â”‚
â”‚  - Match damageâ†’partâ”‚
â”‚  - Generate report  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
    Results
```

### Option 2: Unified Transformer (Experimental)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Image     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Unified Transformer (DETR-like)â”‚
â”‚  - Patch embedding              â”‚
â”‚  - Encoder (6 layers)           â”‚
â”‚  - Decoder (6 layers)           â”‚
â”‚  - Multi-task heads:            â”‚
â”‚    â€¢ Bounding boxes             â”‚
â”‚    â€¢ Vehicle part class         â”‚
â”‚    â€¢ Damage type                â”‚
â”‚    â€¢ Damage location            â”‚
â”‚    â€¢ Severity score             â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â–¼
    Results
```

---

## ğŸ“Š Dataset

### Statistics
- **Total Images**: 33,214 (full dataset)
- **Annotated Subset**: 2,500 images
- **Total Annotations**: 8,118 damage instances
- **Train/Val/Test Split**: 70% / 15% / 15%

### Damage Distribution
| Type | Count | Percentage |
|------|-------|-----------|
| Scratch | 5,834 | 71.9% |
| Broken part | 632 | 7.8% |
| Dent | 560 | 6.9% |
| Cracked | 432 | 5.3% |
| Paint chip | 236 | 2.9% |
| Missing part | 216 | 2.7% |
| Corrosion | 150 | 1.8% |
| Flaking | 58 | 0.7% |

### Most Damaged Parts
1. Front-bumper (29.8%)
2. Back-bumper (18.9%)
3. Front-door (10.6%)
4. Fender (6.1%)

---

## ğŸ”§ Installation

### Prerequisites
- Python 3.8+
- CUDA 11.8+ (for GPU support)
- 16GB+ RAM
- 50GB+ disk space

### Quick Start

```bash
# Clone the repository
git clone https://github.com/YOUR_USERNAME/collision-damage-detection.git
cd collision-damage-detection

# Create virtual environment
python -m venv collision_parts_env
source collision_parts_env/bin/activate  # On Windows: collision_parts_env\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Download pretrained models (if available)
# python download_models.py
```

### Dependencies
```
torch>=2.0.0
torchvision>=0.15.0
ultralytics>=8.0.0
opencv-python>=4.8.0
Pillow>=10.0.0
numpy>=1.24.0
fastapi>=0.100.0
uvicorn>=0.23.0
mlflow>=2.5.0
hydra-core>=1.3.0
loguru>=0.7.0
```

---

## ğŸš€ Usage

### 1. Validate COCO Annotations

```bash
cd working
python validate_coco.py
```

### 2. Split Dataset

```bash
python split_coco_dataset.py
```

### 3. Batch Processing (Generate Annotations)

```bash
# Process first 2500 images
python prepare_2500_dataset.py
python batch_to_coco_2500.py
```

### 4. Train Model

#### Option A: Train Unified Transformer
```bash
cd working
python src/training/train_unified_transformer.py
```

#### Option B: Train ResNet Classifier
```bash
python src/training/train_resnet.py
```

### 5. Run Inference

```bash
# Single image
python src/infer/unified_transformer_inference.py --image path/to/car.jpg

# Batch inference
python batch_to_coco_2500.py --dataset-dir data/processed/yolo/images_2500
```

### 6. Start API Server

```bash
cd working
python app_batch.py
```

Then open: http://localhost:7860

---

## ğŸ“š API Reference

### POST /predict
Predict damages from a single image.

**Request:**
```bash
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@car_image.jpg"
```

**Response:**
```json
{
  "damages": [
    {
      "type": "Scratch",
      "location": "Front-bumper",
      "severity": 6.5,
      "bbox": [150, 200, 80, 60],
      "confidence": 0.92,
      "segmentation": [[150, 200, 230, 200, 230, 260, 150, 260]]
    }
  ],
  "processing_time_ms": 245,
  "model_version": "v1.0"
}
```

---

## ğŸ“ Project Structure

```
collision-damage-detection/
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ config.yaml              # Main config
â”‚   â”œâ”€â”€ correction_codes.yaml    # Damage correction codes
â”‚   â”œâ”€â”€ thresholds.yaml          # Detection thresholds
â”‚   â””â”€â”€ train/                   # Training configs
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed/yolo/
â”‚   â”‚   â”œâ”€â”€ images_2500/         # Selected 2500 images
â”‚   â”‚   â””â”€â”€ batch_output/
â”‚   â”‚       â””â”€â”€ splits/          # Train/Val/Test splits
â”‚   â”‚           â”œâ”€â”€ annotations_train.json
â”‚   â”‚           â”œâ”€â”€ annotations_val.json
â”‚   â”‚           â””â”€â”€ annotations_test.json
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data/                    # Data processing
â”‚   â”œâ”€â”€ models/                  # Model architectures
â”‚   â”‚   â””â”€â”€ unified_transformer.py
â”‚   â”œâ”€â”€ training/                # Training scripts
â”‚   â”‚   â”œâ”€â”€ train_unified_transformer.py
â”‚   â”‚   â”œâ”€â”€ train_resnet.py
â”‚   â”‚   â””â”€â”€ train_yolo.py
â”‚   â”œâ”€â”€ infer/                   # Inference pipeline
â”‚   â””â”€â”€ utils/                   # Utilities
â”œâ”€â”€ working/                     # Experimental scripts
â”‚   â”œâ”€â”€ app_batch.py            # Gradio web interface
â”‚   â”œâ”€â”€ batch_to_coco_2500.py  # Batch annotation generator
â”‚   â”œâ”€â”€ validate_coco.py        # Validation script
â”‚   â”œâ”€â”€ split_coco_dataset.py  # Dataset splitter
â”‚   â””â”€â”€ PROJECT_STATUS.md       # Current status
â”œâ”€â”€ tests/                       # Unit tests
â”œâ”€â”€ docs/                        # Documentation
â”œâ”€â”€ requirements.txt             # Python dependencies
â”œâ”€â”€ dvc.yaml                     # DVC pipeline
â””â”€â”€ README.md                    # This file
```

---

## ğŸ“ˆ Results

### Model Performance (Expected)

| Model | Accuracy | Inference Time | Model Size |
|-------|----------|----------------|------------|
| YOLO11m + ResNet50 | 92-95% | 30-50ms (GPU) | 250MB |
| Unified Transformer | 88-93% | 50-100ms (GPU) | 350MB |

### Training Time
- **YOLO11m**: 2-3 days (GPU) / 5-7 days (CPU)
- **ResNet50**: 1-2 days (GPU) / 3-5 days (CPU)
- **Unified Transformer**: 3-5 days (GPU) / 7-10 days (CPU)

---

## ğŸ› ï¸ Configuration

Edit `configs/config.yaml` to customize:

```yaml
# Detection thresholds
damage_threshold: 0.4    # 40% confidence
part_threshold: 0.3      # 30% confidence
overlap_threshold: 0.4   # 40% IoU for matching

# Training
epochs: 50
batch_size: 16
learning_rate: 0.001
image_size: 448

# Model
model_type: "unified_transformer"  # or "yolo_resnet"
model_size: "small"  # tiny, small, base, large
```

---

## ğŸ§ª Testing

```bash
# Run all tests
python run_tests.py all

# Run specific test suites
python run_tests.py unit
python run_tests.py integration
python run_tests.py api

# Check code quality
python run_tests.py quality

# Generate coverage report
python run_tests.py coverage
```

---

## ğŸ“ Training Your Own Model

### 1. Prepare Your Dataset

```bash
# Organize images
data/raw/your_images/
â”œâ”€â”€ image001.jpg
â”œâ”€â”€ image002.jpg
â””â”€â”€ ...

# Generate COCO annotations (if you have labels)
python working/batch_to_coco.py --dataset-dir data/raw/your_images
```

### 2. Configure Training

Edit `configs/train/unified_transformer.yaml`:

```yaml
model:
  name: "unified_transformer"
  size: "small"  # Faster training

train:
  epochs: 50
  batch_size: 8
  learning_rate: 0.0001
```

### 3. Start Training

```bash
python src/training/train_unified_transformer.py
```

### 4. Monitor Progress

```bash
# Open MLflow UI
mlflow ui

# Navigate to: http://localhost:5000
```

---

## ğŸ³ Docker Deployment

```bash
# Build image
docker build -t collision-damage-api -f docker/Dockerfile.infer .

# Run container
docker run -p 8000:8000 collision-damage-api

# Test
curl http://localhost:8000/health
```

---

## ğŸ¤ Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ™ Acknowledgments

- **Ultralytics YOLO** - Object detection framework
- **PyTorch** - Deep learning framework
- **DETR** - Detection Transformer inspiration
- **MS COCO** - Annotation format standard

---

## ğŸ“ Contact

- **Project Lead**: Your Name
- **Email**: your.email@example.com
- **GitHub**: [@yourusername](https://github.com/yourusername)

---

## ğŸ—ºï¸ Roadmap

- [x] COCO annotation generation (2,500 images)
- [x] Dataset splitting (Train/Val/Test)
- [ ] Train Unified Transformer model
- [ ] Model evaluation and benchmarking
- [ ] FastAPI production deployment
- [ ] Real-time inference optimization
- [ ] Mobile app integration
- [ ] Multi-language support

---

## â­ Star History

If you find this project useful, please consider giving it a star!

[![Star History Chart](https://api.star-history.com/svg?repos=YOUR_USERNAME/collision-damage-detection&type=Date)](https://star-history.com/#YOUR_USERNAME/collision-damage-detection&Date)

---

**Made with â¤ï¸ for safer roads and smarter insurance**
