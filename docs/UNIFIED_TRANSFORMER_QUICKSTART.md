# Unified Transformer - Quick Start Guide

Get started with the unified transformer model in 5 minutes!

## Prerequisites

- Python 3.9+
- PyTorch 2.0+
- CUDA-capable GPU (recommended, 8GB+ VRAM)
- Annotated collision damage dataset

## Installation

```bash
# Clone repository
git clone <your-repo-url>
cd collision-parts-prediction

# Install dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install -r requirements.txt
```

## Quick Training

### 1. Prepare Your Data

Ensure your data is in COCO format with damage attributes:

```json
{
  "images": [...],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "category_id": 0,
      "bbox": [100, 200, 150, 100],
      "damage_location_id": 5,
      "damage_severity": 6,
      "damage_type_id": 2
    }
  ],
  "categories": [...]
}
```

Place in: `data/labelme/annotations.json`

### 2. Train the Model

```bash
# Basic training (uses default config)
python src/training/train_unified_transformer.py

# Custom training
python src/training/train_unified_transformer.py \
  train.model_size=small \
  train.training.batch_size=8 \
  train.training.learning_rate=0.0001
```

**Expected training time:**
- Small dataset (1K images): 2-3 hours
- Medium dataset (5K images): 8-10 hours
- Large dataset (10K+ images): 16-24 hours

### 3. Monitor Training

```bash
# In another terminal
mlflow ui --backend-store-uri mlruns
# Open http://localhost:5000
```

## Quick Inference

### Single Image

```bash
python src/infer/unified_transformer_inference.py \
  --model models/unified_transformer_best.pth \
  --image test_images/damaged_car.jpg \
  --report
```

**Output:**
```
================================================================================
UNIFIED TRANSFORMER - COLLISION DAMAGE ASSESSMENT REPORT
================================================================================
Image: test_images/damaged_car.jpg

Total Detections: 3
Image Size: 1920x1080

--------------------------------------------------------------------------------

ðŸ” Detection #1:
  ðŸ“¦ Bounding Box:
     Position: (450.5, 120.3) â†’ (650.6, 270.7)
     Size: 200.1 Ã— 150.4
  ðŸš— Vehicle Part: front_bumper
     Confidence: 94.5%
  ðŸ“ Damage Location: front_bumper
     Confidence: 92.3%
  âš ï¸  Damage Severity: 6.2/10 (Moderate)
  ðŸ”¨ Damage Type: dent
     Confidence: 88.3%
--------------------------------------------------------------------------------
...
```

### Batch Processing

```python
from pathlib import Path
from PIL import Image
from src.infer.unified_transformer_inference import UnifiedTransformerInference

# Initialize
pipeline = UnifiedTransformerInference(
    model_path="models/unified_transformer_best.pth",
    conf_threshold=0.5
)

# Process all images in directory
image_dir = Path("test_images")
for img_path in image_dir.glob("*.jpg"):
    image = Image.open(img_path)
    results = pipeline.predict(image)
    print(f"{img_path.name}: {results['num_detections']} detections")
```

## Python API

### Basic Usage

```python
import torch
from PIL import Image
from src.models.unified_transformer import create_unified_transformer

# Create model
model = create_unified_transformer(
    num_vehicle_parts=10,
    num_damage_locations=21,
    num_damage_types=11,
    model_size="base"
)

# Load checkpoint
checkpoint = torch.load("models/unified_transformer_best.pth")
model.load_state_dict(checkpoint['model_state_dict'])
model.eval()

# Prepare image
from torchvision import transforms
transform = transforms.Compose([
    transforms.Resize((640, 640)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

image = Image.open("test.jpg")
image_tensor = transform(image).unsqueeze(0)

# Run inference
with torch.no_grad():
    predictions = model(image_tensor)

# Process results
bboxes = predictions['bbox_pred'][0]  # [100, 4]
classes = predictions['class_logits'][0].softmax(dim=-1)  # [100, 11]
locations = predictions['location_logits'][0].softmax(dim=-1)  # [100, 21]
severity = predictions['severity_pred'][0]  # [100, 1]
types = predictions['type_logits'][0].softmax(dim=-1)  # [100, 11]

# Filter confident detections
conf_threshold = 0.5
max_conf, _ = classes[:, :-1].max(dim=-1)  # Exclude "no object" class
valid_mask = max_conf > conf_threshold
print(f"Found {valid_mask.sum()} detections")
```

## Configuration

### Model Sizes

Choose based on your hardware and accuracy needs:

```bash
# Tiny - Fast, lowest accuracy
python src/training/train_unified_transformer.py train.model_size=tiny

# Small - Balanced
python src/training/train_unified_transformer.py train.model_size=small

# Base - Recommended (default)
python src/training/train_unified_transformer.py train.model_size=base

# Large - Best accuracy, slowest
python src/training/train_unified_transformer.py train.model_size=large
```

### Loss Weights

Adjust task importance in `configs/config.yaml`:

```yaml
train:
  loss_weights:
    bbox: 5.0          # Bounding box accuracy
    class: 1.0         # Vehicle part classification
    location: 0.5      # Damage location
    severity: 0.3      # Severity regression
    damage_type: 0.5   # Damage type
```

**Tuning tips:**
- Increase `bbox` if detection is poor
- Increase `severity` if severity predictions are inaccurate
- Increase `location` or `damage_type` if those classifications are weak

### Training Hyperparameters

```yaml
train:
  training:
    batch_size: 8         # Reduce if GPU OOM
    learning_rate: 0.0001 # Lower for stable training
    epochs: 100           # More epochs for larger datasets
    patience: 10          # Early stopping patience
```

## Troubleshooting

### GPU Out of Memory

```bash
# Reduce batch size
python src/training/train_unified_transformer.py train.training.batch_size=4

# Use smaller model
python src/training/train_unified_transformer.py train.model_size=small

# Use gradient accumulation (edit training script)
```

### Poor Detection Performance

1. **Check data quality**: Verify annotations are correct
2. **Increase bbox loss weight**: `train.loss_weights.bbox=10.0`
3. **More queries**: Increase `num_queries` in model config
4. **Longer training**: Increase epochs

### Poor Classification Performance

1. **Check label distribution**: Ensure balanced classes
2. **Increase task loss weights**: Adjust `location`, `severity`, `damage_type`
3. **Add data augmentation**: Enable in config
4. **Use class weights**: For imbalanced datasets

### Slow Training

1. **Use smaller model**: `train.model_size=small` or `tiny`
2. **Reduce image size**: `train.image.size=512`
3. **Increase batch size**: If GPU memory allows
4. **Enable mixed precision**: Already enabled by default

## Performance Tips

### For Training

```yaml
# Fast training config
train:
  model_size: small
  training:
    batch_size: 16
    epochs: 50
  image:
    size: 512
```

### For Inference

```python
# Optimize for speed
model.eval()
torch.backends.cudnn.benchmark = True  # Auto-tune kernels

# Batch inference
images = torch.stack([transform(img) for img in image_list])
with torch.cuda.amp.autocast():  # Mixed precision
    predictions = model(images)
```

## Next Steps

1. **Fine-tune on your data**: Train longer for better accuracy
2. **Export to ONNX**: For production deployment
3. **Integrate with API**: See `INFERENCE_GUIDE.md`
4. **Optimize inference**: TensorRT, quantization
5. **Compare with traditional pipeline**: Evaluate trade-offs

## Resources

- **Full Documentation**: `docs/UNIFIED_TRANSFORMER.md`
- **Architecture Details**: See architecture diagram in docs
- **API Reference**: `docs/API.md`
- **Training Guide**: `docs/TRAINING.md`

## Support

For issues or questions:
1. Check `docs/UNIFIED_TRANSFORMER.md` for detailed info
2. Review training logs in MLflow
3. Examine model predictions with visualization
4. Open an issue on GitHub

## Example Results

**Before (Traditional Pipeline):**
- Inference time: 45ms
- Two models: 175M params
- Separate detection and classification

**After (Unified Transformer):**
- Inference time: 28ms (38% faster)
- Single model: 150M params (14% smaller)
- End-to-end joint optimization
- Better classification accuracy

---

**Ready to train?** Run:
```bash
python src/training/train_unified_transformer.py
```

Good luck! ðŸš€
